<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>GENEA Challenge 2022</title>

  <!-- Bootstrap core CSS -->
  <link href="/2022/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet">
  <link href="/2022/vendor/fontawesome-free/css/all.min.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="/2022/css/iva.min.css" rel="stylesheet">

</head>

<body id="page-top">
  <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
      <span class="d-block d-lg-none">GENEA Challenge 2022</span>
      <!-- <span class="d-none d-lg-block">
        <img src="img/avatar.png" class="img-fluid img-profile rounded-circle mx-auto mb-5" alt="">

      </span> -->
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#home">Home</a>
        </li>
	<li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#important-dates">Important dates</a>
        </li>
    
    <li class="nav-item">
      <a class="nav-link js-scroll-trigger" href="#workshop-programme">Challenge programme</a>
    </li>
    
        
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#call-for-participation">Call for participation</a>
        </li>
	<li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#rules">Challenge Rules</a>
        </li>

        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#registration">Registration</a>
        </li>
        
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#accepted-papers">Challenge papers</a>
        </li>
        
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#reproducibility-award">Reproducibility Award</a>
        </li>
        
        
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#data-n-proceedings">Data and proceedings</a>
        </li>
        
       
        <!--

        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#invited-speakers">Invited speakers</a>
        </li>-->
        
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#organising-committee">Organising committee</a>
        </li>
        <!--

        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#program-committee">Program committee</a>
        </li>-->

      </ul>
    </div>
  </nav>

  <div class="container-fluid p-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="home">
      <div class="w-100">
        <div class="row">
          <div class="col-md-9 col-sm-12">
            <h1 class="mb-2">GENEA Challenge 2022
            </h1>

            <div class="subheading mb-5">Generation and Evaluation of Non-verbal Behaviour for Embodied Agents</div>
            <!--  -->

            <p class="mb-5">
              <p> Challenge results now available! Results and materials from the Challenge can be found on <a href="https://youngwoo-yoon.github.io/GENEAchallenge2022"> the main GENEA Challenge 2022 results page</a>.
              
              <p>The <b>GENEA Challenge 2022 on speech-driven gesture generation</b> aims to bring together researchers that use different methods for non-verbal-behaviour generation and evaluation, and hopes to stimulate the discussions on how to improve both the generation methods and the evaluation of the results.</p>

              <p>The results of the challenge will be presented at <a href="https://genea-workshop.github.io/2022/workshop/" target="_blank">the 3rd GENEA workshop </a> at <a href="https://icmi.acm.org/2022/" target="_blank">ACM ICMI 2022</a>, with accepted challenge papers published in the main ICMI proceedings.</p>

              <p>This is the second installment of the GENEA Challenge. You can <a href="https://svito-zar.github.io/GENEAchallenge2020/" target="_blank">read more about the previous GENEA Challenge here</a>.</p>
              <p> This challenge is supported Wallenberg AI, Autonomous Systems and Software Program ( <a href="https://wasp-sweden.org/"> WASP </a> ) funded by the Knut and Alice Wallenberg Foundation with in-kind contribution from the <a href="https://www.ea.com/en-gb"> Electronic Arts (EA)</a> R&D department, <a href="https://seed.ea.com/" target="_blank"> SEED </a>. </p> <br>
            </p>
            <div class="row justify-content-center">

              <img src="/2022/img/avatar.png"  class="img-fluid rounded" class="mt-2" width=300 alt="">
            </div>
          </div>
          <div class="col-md-3 d-none d-md-block">
            <a class="twitter-timeline" data-tweet-limit=3 href="https://twitter.com/WorkshopGenea?ref_src=twsrc%5Etfw" target="_blank">Follow us on twitter <i class="fab fa-twitter"></i></a>
            <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
          </div>
          <div class="col-xs-12 d-md-none mt-4 text-center w-100">
            <a href="https://twitter.com/WorkshopGenea" target="_blank">Follow us on Twitter <i class="fab fa-twitter"></i></a>
          </div>
        </div>
      </div>

    </section>

    <hr class="m-0">

    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="important-dates">
      <div class="w-100">
        <h2 class="mb-5">Important dates</h2>

       <div class="row">
          <div class="col">April 4, 2022</div>
          <div class="col">Registration opens</div>
       </div>

        <div class="row">
          <div class="col">May 16, 2022</div>
          <div class="col">Training dataset released to challenge participants</div>
        </div>
        <div class="row">
          <div class="col">June 20, 2022</div>
          <div class="col">Test inputs released to participants
</div>
        </div>
        <div class="row">
          <div class="col"><b>June 27, 2022</b></div>
          <div class="col"><b>Deadline for</b> participants to submit <b>generated motion</b></div>
        </div>
        <div class="row">
          <div class="col">July 15, 2022</div>
          <div class="col">Release of crowdsourced evaluation results to participants</div>
        </div>
        <div class="row">
          <div class="col"><b>July 22, 2022</b></div>
          <div class="col"><b>Deadline for</b> participants to submit <b>system-description papers</b>
</div>
        </div>
	<div class="row">
          <div class="col">August 10, 2022</div>
          <div class="col">Paper notification</div>
        </div>
        <div class="row">
          <div class="col"><b>August 17, 2022</b></div>
          <div class="col"><b>Deadline for camera-ready papers</b></div>
        </div>
        <div class="row">
          <div class="col"><b>November 11, 2022</b></div>
          <div class="col"><b>Challenge presentations at ICMI 2022 (Virtual)</b></div>
        </div>

    </section>


    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="workshop-programme">
      <div class="w-100">
        <h2 class="mb-5">Planned Challenge programme</h2>
        <h4>All times in India Standard Time</h4>
        <div>Note, all times are in the <a href="https://everytimezone.com/s/b16124c7" target="_blank">India Standard timezone</a>. The Challenge presentations will take place virtually over Zoom on November 11th. <br><br></div>
        <div class="row">
            <div class="col"> <a href="https://everytimezone.com/s/b16124c7" target="_blank">14:30&nbsp;-&nbsp;14:40</a>  </div>
            <div class="col-10">Opening statement</div>
        </div>
        <div class="row">
            <div class="col"><a href="https://everytimezone.com/s/ad7f8507" target="_blank">14:45&nbsp;-&nbsp;16:00</a> </div>
            <div class="col-10">Challenge system presentations</div>
        </div>
        <div class="row">
          <ul style="list-style: none">
            <li><span style="margin-right: 10px;"> </span> <a href="https://arxiv.org/abs/2208.10441v1" target="_blank">The GENEA Challenge 2022: A large evaluation of data-driven co-speech gesture generation </a> by Youngwoo Yoon, Pieter Wolfert, Taras Kucherenko, Carla Viegas, Teodor Nikolov, Mihail Tsakov, Gustav Eje Henter</li>
            <li><span style="margin-right: 10px;"> </span> <a href="https://openreview.net/forum?id=-2HZD-e6pX7W" target="_blank">Hybrid Seq2Seq Architecture for 3D Co-Speech Gesture Generation</a> by Khaled Saleh</li>
            <li><span style="margin-right: 10px;"> </span> <a href="https://openreview.net/forum?id=AYMDEx97qPN" target="_blank">TransGesture: Autoregressive Gesture Generation with RNN-Transducer</a> by Naoshi Kaneko, Yuna Mitsubayashi, Geng Mu</li>
            <li><span style="margin-right: 10px;"> </span> <a href="https://openreview.net/forum?id=atWaELmguNj7" target="_blank"> The ReprGesture entry to the GENEA Challenge 2022</a> by Sicheng Yang, Zhiyong Wu, Minglei Li, Mengchen Zhao, Jiuxin Lin, Liyang Chen, Weihong Bao</li>
            <li><span style="margin-right: 10px;"> </span> <a href="https://openreview.net/forum?id=uX86IlhiHNx" target="_blank"> ReCell: replicating recurrent cell for auto-regressive pose generation</a> by Vladislav Korzun, Anna Beloborodova, Arkady Iliin</li>
          </ul>
        </div>
        <div class="row">
          <div class="col"><a href="https://everytimezone.com/s/5c9fc556" target="_blank">16:00&nbsp;-&nbsp;16:15 </a> </div>
          <div class="col-10">Break (Optional socialisation on gather.town)</div>
        </div>
        <div class="row">
            <div class="col"> <a href="https://everytimezone.com/s/13f193f7" target="_blank">16:15&nbsp;-&nbsp;17:30 </a></div>
            <div class="col-10">Challenge system presentations</div>
        </div>
        <div class="row">
          <ul style="list-style: none">
            <li><span style="margin-right: 10px;"> </span> <a href="https://openreview.net/forum?id=PHadbLGjHRL" target="_blank">GestureMaster: Graph-based Speech-driven Gesture Generation</a> by Chi Zhou, Tengyue Bian, Kang Chen</li>
            <li><span style="margin-right: 10px;"> </span> <a href="https://openreview.net/forum?id=RZP6nErM2Xa" target="_blank">UEA Digital Humans entry to the GENEA Challenge 2022</a> by Jonathan Windle, David Greenwood, Sarah Taylor</li>
            <li><span style="margin-right: 10px;"> </span> <a href="https://openreview.net/forum?id=fO_Q4q1dFAA" target="_blank">Exemplar-based Stylized Gesture Generation from Speech: An Entry to the GENEA challenge 2022</a> by Saeed Ghorbani, Ylva Ferstl, Marc-André Carbonneau</li>
            <li><span style="margin-right: 10px;"> </span> <a href="https://openreview.net/forum?id=gMTaia--AB2" target="_blank"> The IVI Lab entry to the GENEA Challenge 2022 -- A Tacotron2 Based Method for Co-Speech Gesture Generation
                With Locality-Constraint Attention Mechanism</a> by Che-Jui Chang, Sen Zhang, Mubbasir Kapadia</li>
            <li><span style="margin-right: 10px;"> </span> <a href="https://openreview.net/forum?id=zEqdFwAPhhO" target="_blank"> The DeepMotion entry to the GENEA Challenge 2022</a> by Shuhong Lu, Andrew Feng</li>
          </ul>
        </div>
        <div class="row">
          <div class="col"><a href="https://everytimezone.com/s/08118ade" target="_blank">17:30&nbsp;-&nbsp;17:45 </a> </div>
          <div class="col-10">Group discussion</div>
        </div>
        <div class="row">
          <div class="col">17:45&nbsp;-&nbsp;18:00</div>
          <div class="col-10">Reproducibility award and Closing remarks</div>
        </div>
        <div class="row">
          <div class="col">18:00&nbsp;-&nbsp;</div>
          <div class="col-10">Informal mingle</div>
        </div>
      </div>
    </section>


    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="call-for-participation">
      <!-- justify-content-center -->
      <div class="w-100">
        <h2 class="mb-5">Call for participation</h2>

        <div class="iva-item d-flex flex-column flex-md-row justify-content-between mb-5">
          <div class="iva-content">
            
            <p>The state of the art in co-speech gesture generation is difficult to assess since every research group tends to use its own data, embodiment, and evaluation methodology. To better understand and compare methods for gesture generation and evaluation, we are continuing the GENEA (Generation and Evaluation of Non-verbal Behaviour for Embodied Agents) Challenge, wherein different gesture-generation approaches are evaluated side by side in a large user study. This 2022 challenge is a Multimodal Grand Challenge for ICMI 2022 and is a follow-up to the first edition of the GENEA Challenge, arranged in 2020.</p>

            <p>We invite researchers in academia and industry working on any form of corpus-based generation of gesticulation and non-verbal behaviour to submit entries to the challenge, whether their method is driven by rule or machine learning. Participants are provided a large, common dataset of speech (audio+aligned text transcriptions) and 3D motion to develop their systems, and then use these systems to generate motion on given test inputs. The generated motion clips are rendered onto a common virtual agent and evaluated for aspects such as motion quality and appropriateness in a large-scale crowdsourced user study. </p>
 

            <p>The results of the challenge are presented in hybrid format at the 3rd GENEA Workshop at ICMI 2022, together with individual papers describing each participating system. All accepted challenge papers will be published in the main ACM ICMI 2022 proceedings. </p>
            <br>

            
          </div>
        </div>
      </div>
    </section>


    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="rules">
      <!-- justify-content-center -->
      <div class="w-100">
        <h2 class="mb-5">Rules for the GENEA Challenge 2022</h2>
        <div class="iva-item d-flex flex-column flex-md-row justify-content-between mb-5">
          <div class="iva-content">
            Only register for this challenge if you actually intend to submit an entry to the challenge and to comply with all its rules.<br><br>

            <h3>Goal of the challenge</h3>
The GENEA Challenge seeks to advance scientific knowledge relating to automatic gesture generation, by means of open science and a large-scale, joint subjective evaluation. It is therefore a challenge, not a competition: the point is not to find who does best, but what works best. The rules of the challenge have been designed with the intent of best furthering this goal. <br>
            <br>

	    <h3>Database access</h3>
	    The gesture database is currently only available to registered participants in the challenge. Access to the data and participating in the challenge may also require completing and agreeing to a data licence agreement. <br>

	    Download passwords will be issued after your registration is accepted and you have completed any required licences. Do not share the data or passwords with non-challenge participants.<br>
	    <br>

            <h3>Materials provided</h3>
	    All participants who have signed the licence will be given access to the following materials:
            <br>

            <ul>
			  <li> 3D full-body motion-capture clips of a speaking and gesticulating person, in BVH format.</li>
			  <li> Aligned audio waveforms of the speech associated with the motion-capture clips, in WAV format.</li>
			  <li> Text transcripts of each audio file with word-level timing information, in CSV and JSON format.</li>
                          <li> A label coding the identity of the person gesticulating in each recording.</li>
			  <li> Code and scripts for replicating the training of the previously-published baseline systems to be included in the challenge evaluation.</li>
			  <li> A pipeline for visualising their system output as videos of a gesticulating avatar, the same as will be used to render video stimuli for the challenge evaluation.</li>
	    </ul>

	    <p>The above motion, audio, and transcriptions have been partitioned into an official training set and an official validation set. Please respect this split, and do not train on validation data when developing your system. (You may only train on the validation data when creating your final submission.) The official validation set was created using the same process as the held-out test set for the challenge, and has similar duration and other characteristics. It is therefore your best guide to what the final, held-out test set will look like.</p>

            <p>If the full data release to participants is delayed, dummy data files illustrating the folder structure, filenames, and data formats will be made available. This allows participants to set up their data-processing pipelines in advance of the full data release.</p>

            <p>Approximately one week before the deadline to submit generated motion stimuli, participants will also be given access to:</p>

             <ul>
			  <li> Held-out audio waveforms from the same source as the training audio, in WAV format.</li>
			  <li> Text transcripts of each held-out audio file with word-level timing information, in CSV and JSON format.</li>
			  <li> A label coding the identity of the person gesticulating in each recording.</li>
	    </ul>
            <p>The task of the challenge is to use one’s system to generate convincing gesture motion for this held-out speech, and submit that motion for evaluation. For this reason, we will not provide motion data with the held-out speech. Note that not all of the synthetic motion output submitted to the challenge may be included in the final evaluation.</p>



            <p>If, for some reason, you have or gain access to the held-out motion data, we rely on your honesty in not looking at that material or letting it influence your challenge submission. </p>
	    <br>

	    <h3>Limits on participation</h3>
	    <p>Each participating team may only submit one system per team for evaluation. Teams can consist of one or more persons, from zero or more academic institutions and/or commercial entities. </p>

	    <p>Participants involved in joint projects or consortia who wish to submit multiple systems (e.g., an individual entry and a joint system) should contact the organisers in advance and receive approval first. We will try to accommodate all reasonable requests, provided the evaluation remains manageable. If the number of participating teams is small (e.g., less than five), the organisers may decide to permit multiple entries per team.</p>
            <br>
 

	    <h3>Use of external data</h3>
            <ul>
			  <li> “External data” is defined as data, of any type, that is not part of the provided database. This includes, for example, raw recordings, structured databases, and pre-trained systems such as word vectors.</li>
			  <li> For this year's challenge, only open external data – data that is available to the public free of charge (possibly after signing a licence) – may be used.
</li>
			  <li> All external data used in your system must be explicitly acknowledged by providing a citation and/or link in the paper accompanying your submission.</li>
                          <li> You are allowed to use external data in any way you wish, subject to any exclusions or limitations given in these rules.</li>
	    </ul>

	    <p>For <b>data pertaining to text and audio</b>, any external data may be used, as long as they satisfy the criteria above. There is no limitation on the amount of such data you may use.</p>

	    <p>For <b>motion data</b> (whether 2D, 3D, or video), no external motion data is permitted. The reason for this data restriction is that other behaviour-generation challenges have found that system performance often is limited by the amount of training data that can be ingested, which is not an interesting scientific conclusion to replicate.</p>
         
            <p>Your system must make use of the provided motion data, but you may exclude parts of that data if you wish. Use of the provided audio or text transcripts is entirely optional and not compulsory, as is the use of external text and audio data.</p>
         
	    <p>Please keep in mind that the point of the challenge is to gain better insight into the synthesis and perception of motion and gestures, not to see who has the best data and resources. Consequently, participants are strongly encouraged to share processed material they are using in their entries with other participants and with the organisers. Example data that may be valuable to share include: improved transcripts and alignments; motions from permitted external databases converted to the challenge format and retargeted to the challenge skeleton; denoised and reconstructed motion data; sub-selected data; bug fixes to baseline systems; etc. </p>


            <p>If you are in any doubt about how to apply these rules, please contact the organisers for clarification!</p>
            <br>

	    <h3>Synthesising test motion</h3>
	    <p>Synthetic gesture motion must be submitted in the same format as that used by the challenge gesture database (BVH, same skeleton, frame rate, etc.). The organisers take no responsibility for any effects that may occur when processing motion that was not submitted in the correct format. </p>

	    <p>Manually tweaking the output motion is not allowed, since the idea is to evaluate how systems would perform in an unattended setting.</p>
	    <br>

            <h3>Retention and distribution of submitted stimuli</h3>
	    <p>Any stimuli that you submit for evaluation will be retained by the organisers for future use. The evaluated stimuli and any associated user ratings and comments will also be made publicly available for non-commercial purposes, labelled by the corresponding anonymised system label.
 </p>
            <br>

            <h3>Evaluation</h3>
	    <p>The GENEA Challenge centres on subjective human perception, not objective metrics. A large-scale formal evaluation by means of several user studies will be conducted to jointly evaluate and compare the submitted co-speech gestures. These user studies will be carried out online using crowdsourced raters who speak and comprehend the language featured in the database.</p>

	    The evaluation of the submitted gesture motion will likely consider aspects such as:
            <br>

            <ul>
			  <li> Its perceived human-likeness, without accounting for the speech;</li>
			  <li> its appropriateness for the associated held-out speech, in terms of, e.g., timing, semantic content, or both; and</li>
			  <li> its appropriateness for the individual gesticulation style of the indicated test speaker in each segment.</li>
	    </ul>

            <p>The results of the evaluation, including a statistical analysis, will be made public, albeit with the identity of participating systems anonymised. Participating teams will be informed of the results and which system is theirs, so that they can draw conclusions and describe what they learned in papers describing their submissions.</p>
	    <br>

            <h3>Paper</h3>

            <ul>
			  <li> Each participant must submit a paper (using the template specified) describing their challenge entry for double-blind review. This submission and its reviews will be permanently available on OpenReview.</li>
			  <li> Each participant is also expected to complete a form giving the general technical specification of their system, to facilitate easy cross-system comparisons.</li>
			  <li> One of the <b>authors</b> of each accepted paper <b>must</b> register and present their work at the conference workshop associated with the challenge, if their paper is accepted, or if the organisers otherwise ask them to do so.</li>
                           <li> Paper submission, copyright, publishing, and presentation is subject to ACM rules and the rules of the parent conference at which the GENEA Challenge is hosted. Do not enter the challenge if you are unable to comply with these rules.</li>


	    </ul>

            <br>


            <h3>Use of results</h3>

            <p>This gesture-generation challenge is a scientific exercise. You may use the results only for the purpose of scientific research. Specifically, you may not use the results (e.g., your team’s ranking in the evaluations) for any commercial purposes, including but <b>not</b> limited to advertising products or services.</p>

            <br>


            Please contact the organisers at genea-contact@googlegroups.com if you have any questions about these rules.

            
          </div>
        </div>
      </div>
    </section>


   <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="registration">
      <!-- justify-content-center -->
      <div class="w-100">
        <h2 class="mb-5">Registration</h2>

        <div class="iva-item d-flex flex-column flex-md-row justify-content-between mb-5">
          <div class="iva-content">
            
            Challenge registration is open!<br><br>

            <p>Please make sure to read and agree to <a href="https://genea-workshop.github.io/2022/challenge/#rules" target="_blank">the challenge rules</a> before registering. Do not register for the challenge if you do not intend to comply with these rules. </p>

            <p>Once you have read the rules, please <a href="https://forms.gle/5ho6pnZiFrMeo3yM9" target="_blank">use this sign-up form to register your team</a>.</p>

            <br>

            
          </div>
        </div>
      </div>
    </section>


    <hr class="m-0">
    
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="accepted-papers">
      <div class="w-100">
        <h2 class="mb-5">Challenge papers</h2>
        
               <p>
                  <h4><a href="https://openreview.net/pdf?id=PHadbLGjHRL" target="_blank">GestureMaster: Graph-based Speech-driven Gesture Generation</a></h4>
                  <i>Chi Zhou, Tengyue Bian, Kang Chen</i><br><br>
                </p>
                  <hr>
                <p>
                  <h4><a href="https://openreview.net/pdf?id=RZP6nErM2Xa" target="_blank">UEA Digital Humans entry to the GENEA Challenge 2022</a></h4>
                  <i>Jonathan Windle, David Greenwood, Sarah Taylor</i><br><br>
                </p>
                <hr>
                <p>
                   <h4><a href="https://openreview.net/pdf?id=atWaELmguNj7" target="_blank">The ReprGesture entry to the GENEA Challenge 2022 </a></h4>
                   <i>Sicheng Yang, Zhiyong Wu, Minglei Li, Mengchen Zhao, Jiuxin Lin, Liyang Chen, Weihong Bao</i><br><br>
                 </p>
                 <hr>
                 <p>
                    <h4><a href="https://openreview.net/pdf?id=zEqdFwAPhhO" target="_blank">The DeepMotion entry to the GENEA Challenge 2022</a></h4>
                    <i>Shuhong Lu, Andrew Feng</i><br><br>
                  </p>
                  
                <hr>
                <p>
                  <h4><a href="https://openreview.net/pdf?id=-2HZD-e6pX7W" target="_blank">Hybrid Seq2Seq Architecture for 3D Co-Speech Gesture Generation</a></h4>
                  <i>Khaled Saleh</i><br><br>
                </p>
                <hr>
                <p>
                   <h4><a href="https://openreview.net/pdf?id=gMTaia--AB2" target="_blank">The IVI Lab entry to the GENEA Challenge 2022 -- A Tacotron2 Based Method for Co-Speech Gesture Generation With Locality-Constraint Attention Mechanism</a></h4>
                   <i>Che-Jui Chang, Sen Zhang, Mubbasir Kapadia</i><br><br>
                 </p>
                 <hr>
                 <p>
                    <h4><a href="https://openreview.net/pdf?id=AYMDEx97qPN" target="_blank">TransGesture: Autoregressive Gesture Generation with RNN-Transducer</a></h4>
                    <i>Naoshi Kaneko, Yuna Mitsubayashi, Geng Mu</i><br><br>
                  </p>
                  <hr>
                  <p>
                     <h4><a href="https://openreview.net/pdf?id=fO_Q4q1dFAA" target="_blank">Exemplar-based Stylized Gesture Generation from Speech: An Entry to the GENEA Challenge 2022</a></h4>
                     <i>Saeed Ghorbani, Ylva Ferstl, Marc-André Carbonneau</i><br><br>
                   </p>
                   <hr>
                   <p>
                      <h4><a href="https://openreview.net/pdf?id=uX86IlhiHNx" target="_blank">ReCell: replicating recurrent cell for auto-regressive pose generation</a></h4>
                      <i>Vladislav Korzun, Anna Beloborodova, Arkady Iliin</i><br><br>
                    </p>
                  

    </div>
    </section>

    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="reproducibility-award">
      <div class="w-100">
        <h2 class="mb-5">Reproducibility Award</h2>
        Reproducibility is a cornerstone of the scientific method. Lack of reproducibility is a serious issue in contemporary research which we want to address at our workshop. To encourage authors to make their papers reproducible, and to reward the effort that reproducibility requires, we are introducing the GENEA Workshop Reproducibility Award. All short and long papers presented at the GENEA Workshop will be eligible for this award. Please note that it is the camera-ready version of the paper which will be evaluated for the reward.
        <br><br>
        The award is awarded to the paper with the greatest degree of reproducibility. The assessment criteria include:
        <ul>
          <li>ease of reproduction (ideal: just works, if there is code - it is well documented and we can run it)</li>
          <li>extent (ideal: all results can be verified)</li>
          <li>data accessibility (ideal: all data used is publicly available)</li>
        </ul>
        <div style="text-align: center; margin-top: 30px;">
                  This year's award is awarded to: <b>The IVI Lab entry to the GENEA Challenge 2022 – A Tacotron2 Based Method for Co-Speech Gesture Generation With Locality-Constraint Attention Mechanism</b> <br> by
                  <i>Che-Jui Chang, Sen Zhang, Mubbasir Kapadia</i>. <br><br>
                  <img src="/2022/img/GENEA_challenge_reproducibility_award.png" alt="reproducibility award" width=600>
        </div>
      </div>
      
    </section>

    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="data-n-proceedings">
      <div class="w-100">
        <h2 class="mb-5">Data and proceedings</h2>
	<p> All the materials derived from the Challenge can be found on <a href="https://youngwoo-yoon.github.io/GENEAchallenge2022"> our official website </a>.  </p>
      </div>

    </section>

    <hr class="m-0">
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="organising-committee">
      <div class="w-100">
        <h2 class="mb-5">Organising committee</h2>
        <p>
          The main contact address of the workshop is: <a
            href="mailto:genea-contact@googlegroups.com">genea-contact@googlegroups.com</a>. <br> <br>
          </p>
        <h4>Workshop organisers</h4>

        <div class="row">


          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2022/img/pieter.jpeg" width="100%" class="img-fluid rounded" alt="Pieter Wolfert">
              </div>
              <div class="col-7">
                  <a href="https://www.pieterwolfert.com" target="_blank" style="font-weight: bold;">Pieter Wolfert</a>
                  <br>
                  IDLab, Ghent University - imec <br> Belgium
              </div>
            </div>
            <hr>
          </div>

          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2022/img/taras.jpg" class="img-fluid rounded" alt="Taras Kucherenko">
              </div>
              <div class="col-7">
                <a href="https://svito-zar.github.io/" target="_blank" style="font-weight: bold;">Taras Kucherenko</a>
                <br>
                Electronic Arts (EA) <br> Sweden

              </div>
            </div>
            <hr>
          </div>
          

        </div>

        <div class="row">
          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2022/img/youngwoo.jpg" class="img-fluid rounded" alt="Youngwoo Yoon">
              </div>
              <div class="col-7">

                <a href="https://sites.google.com/view/youngwoo-yoon/" target="_blank" style="font-weight: bold;">Youngwoo Yoon</a>
                <br>ETRI <br> South Korea


              </div>
            </div>
            <hr>
          </div>
          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2022/img/carla.jpg" class="img-fluid rounded" alt="Carla Viegas">
              </div>
              <div class="col-7">
                  <a href="https://www.carlaviegas.info" target="_blank" style="font-weight: bold;">Carla Viegas</a>
                  <br>
                  Carnegie Mellon University <br> United States of America
              </div>
            </div>
            <hr>
          </div>
        </div>

        <div class="row">
          <div class="col-sm-12 col-md-6">
            <div class="row">
              <div class="col-5">
                <img src="/2022/img/gustav.jpeg" class="img-fluid rounded" alt="Gustav Eje Henter">
              </div>
              <div class="col-7">

                <a href="https://people.kth.se/~ghe/" target="_blank" style="font-weight: bold;">Gustav Eje Henter</a>
                <br>
                KTH Royal Institute of Technology <br> Sweden


              </div>
            </div>
            <hr>
          </div>

        </div>

      </div>
    </section>
    <hr class="m-0">

    <!--
    <section class="iva-section p-3 p-lg-5 d-flex align-items-center" id="program-committee">
      <div class="w-100">
        <h2 class="mb-5">Program committee</h2>
        TBD
      </div>

    </section>
    <hr class="m-0">
    --->


  </div>

  <!-- Bootstrap core JavaScript -->
  <script src="/2022/vendor/jquery/jquery.min.js"></script>
  <script src="/2022/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="/2022/vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="/2022/js/iva.min.js"></script>

</body>

<!-- Panelbear -->
<script async src="https://cdn.panelbear.com/analytics.js?site=9bGH0f0hxBy"></script>
<script>
    window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    panelbear('config', { site: '9bGH0f0hxBy' });
</script>

</html>
